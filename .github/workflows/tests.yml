name: PlexAudioConverter Strong Test System

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'state_management/**'
      - 'core/**'
      - 'tests/**'
      - 'pytest.ini'
      - '.coveragerc'
      - '.github/workflows/tests.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'state_management/**'
      - 'core/**'
      - 'tests/**'
      - 'pytest.ini'
      - '.coveragerc'
  workflow_dispatch:
    inputs:
      include_slow_tests:
        description: 'Include slow performance tests'
        required: false
        default: false
        type: boolean

jobs:
  test-matrix:
    name: Test Suite (${{ matrix.os }}, Python ${{ matrix.python-version }})
    runs-on: ${{ matrix.os }}
    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest]
        python-version: ['3.8', '3.9', '3.10', '3.11', '3.12']
        
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        
    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
        
    - name: Install system dependencies (Windows)
      if: matrix.os == 'windows-latest'
      run: |
        # Install chocolatey if not present
        if (!(Get-Command choco -ErrorAction SilentlyContinue)) {
          Set-ExecutionPolicy Bypass -Scope Process -Force
          [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072
          iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))
        }
        choco install ffmpeg -y
      shell: powershell
      
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -r requirements.txt
        pip install -r tests/requirements.txt
        
    - name: Create test results directory
      run: |
        mkdir -p test_results/coverage
      shell: bash
        
    - name: Run Step 2 Core Tests
      run: |
        python tests/run_tests.py --step2 --verbose
      env:
        COVERAGE_FILE: test_results/coverage/.coverage.step2
        
    - name: Run Platform Edge Case Tests  
      run: |
        python tests/run_tests.py --platform --verbose
      env:
        COVERAGE_FILE: test_results/coverage/.coverage.platform
        
    - name: Run Reliability Tests
      run: |
        python tests/run_tests.py --reliability --verbose
      env:
        COVERAGE_FILE: test_results/coverage/.coverage.reliability
        
    - name: Run Integration Tests
      run: |
        python tests/run_tests.py --integration --verbose
      env:
        COVERAGE_FILE: test_results/coverage/.coverage.integration
        
    - name: Run Future Step Tests (Expected Skip/XFail)
      run: |
        python tests/run_tests.py --future --verbose
      continue-on-error: true  # These are expected to be skipped
      
    - name: Run Performance Tests
      if: github.event.inputs.include_slow_tests == 'true' || matrix.python-version == '3.11'
      run: |
        python tests/run_tests.py --performance --verbose --include-slow
      continue-on-error: true  # Performance tests may be flaky
      
    - name: Combine Coverage Reports
      run: |
        coverage combine test_results/coverage/.coverage.*
        coverage report --show-missing
        coverage html -d test_results/coverage/html
        coverage xml -o test_results/coverage/coverage.xml
        coverage json -o test_results/coverage/coverage.json
        
    - name: Upload Coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: test_results/coverage/coverage.xml
        flags: unittests
        name: codecov-${{ matrix.os }}-py${{ matrix.python-version }}
        fail_ci_if_error: false
        
    - name: Archive Test Results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}
        path: |
          test_results/
          !test_results/coverage/.coverage*
          
    - name: Archive Coverage HTML Report
      uses: actions/upload-artifact@v3  
      if: always()
      with:
        name: coverage-html-${{ matrix.os }}-py${{ matrix.python-version }}
        path: test_results/coverage/html/

  test-summary:
    name: Test Results Summary
    runs-on: ubuntu-latest
    needs: test-matrix
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Download all test artifacts
      uses: actions/download-artifact@v3
      with:
        path: all-test-results/
        
    - name: Install analysis dependencies
      run: |
        pip install jq json2table
        
    - name: Generate Test Summary
      run: |
        echo "# Test Results Summary" > $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| OS | Python | Step2 | Platform | Reliability | Integration | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|---|---|---|---|---|---|---|" >> $GITHUB_STEP_SUMMARY
        
        # Process each test result
        for dir in all-test-results/test-results-*; do
          if [ -d "$dir" ]; then
            os_py=$(basename "$dir" | sed 's/test-results-//')
            
            # Extract results from JSON reports if available
            step2_status="â“"
            platform_status="â“"
            reliability_status="â“"
            integration_status="â“"
            overall_status="â“"
            
            if [ -f "$dir/test_summary.json" ]; then
              # Parse summary JSON for status
              summary_file="$dir/test_summary.json"
              
              if command -v jq >/dev/null 2>&1; then
                step2_passed=$(jq -r '.suite_results."Step2 Core".success // false' "$summary_file")
                platform_passed=$(jq -r '.suite_results."Platform Edge Cases".success // false' "$summary_file")
                reliability_passed=$(jq -r '.suite_results."Reliability".success // false' "$summary_file")
                integration_passed=$(jq -r '.suite_results."Integration".success // false' "$summary_file")
                
                [ "$step2_passed" = "true" ] && step2_status="âœ…" || step2_status="âŒ"
                [ "$platform_passed" = "true" ] && platform_status="âœ…" || platform_status="âŒ" 
                [ "$reliability_passed" = "true" ] && reliability_status="âœ…" || reliability_status="âŒ"
                [ "$integration_passed" = "true" ] && integration_status="âœ…" || integration_status="âŒ"
                
                # Overall status
                if [ "$step2_passed" = "true" ] && [ "$platform_passed" = "true" ] && 
                   [ "$reliability_passed" = "true" ] && [ "$integration_passed" = "true" ]; then
                  overall_status="âœ…"
                else
                  overall_status="âŒ"
                fi
              fi
            fi
            
            echo "| $os_py | $step2_status | $platform_status | $reliability_status | $integration_status | $overall_status |" >> $GITHUB_STEP_SUMMARY
          fi
        done
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Acceptance Criteria (Step 2)" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… All **Core Step 2 tests** (T-001 to T-012) are **green** on CI for Linux and Windows" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… No measurable CPU spin in idle scenarios; due scheduling is the only wake-up trigger" >> $GITHUB_STEP_SUMMARY
        echo "- âœ… State is fully recoverable between restarts; no duplicate entries" >> $GITHUB_STEP_SUMMARY
        
    - name: Check Critical Test Failures
      run: |
        # Fail the workflow if Step 2 core tests failed on any platform
        failed_critical=0
        
        for dir in all-test-results/test-results-*; do
          if [ -d "$dir" ] && [ -f "$dir/test_summary.json" ]; then
            if command -v jq >/dev/null 2>&1; then
              step2_passed=$(jq -r '.suite_results."Step2 Core".success // false' "$dir/test_summary.json")
              if [ "$step2_passed" != "true" ]; then
                echo "âŒ Critical failure: Step 2 Core tests failed on $(basename "$dir")"
                failed_critical=1
              fi
            fi
          fi
        done
        
        if [ $failed_critical -eq 1 ]; then
          echo "ğŸ’¥ CRITICAL: Step 2 core tests failed - this blocks the acceptance criteria"
          exit 1
        else
          echo "ğŸ‰ SUCCESS: All Step 2 core tests passed across platforms!"
        fi